{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e15e8538",
   "metadata": {},
   "source": [
    "#### Multiarm Bandit algorithms?\n",
    "\n",
    "Problem :- How an idealized gambler would try to make as much money as possible in hypothetical casino?\n",
    "\n",
    "In this hypothetical casino, there's only one type of game: a slot machine, which is also sometimes called a one-argmed bandit because of its propensity to take your money. While this casino only features slot machines, it could still be an interesting place to visit because there are many different slot machines, each of which has a different payout schedule.\n",
    "\n",
    "For whatever reason, the original mathematicians decided to treat different slot machines in their thought experiment as if they were were one giant slot machine that had many arms. This led them to refer to the options in their problem as arms. It also led them to call this thought experiment the Multiarm Bandit Problem.\n",
    "\n",
    "Reward: A measure of success; it might tell us whether a customer clicked on an ad or signed up as a user.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16de988",
   "metadata": {},
   "source": [
    "#### What's a Bandit Problem?\n",
    "- We are facing a complicated slot machine, called a bandit, that has a set of N arms that we can pull on.\n",
    "- When pulled, any given arm will output a reward. But there rewards are not reliable, which is why we were gambling: Arm I might give us 1 unit of reward only 1% of the time, whiLE Arm 2 might give us 1 unit of reward only 3% of the time. Any specific pull of any specific arm is risky.\n",
    "- Not only is each pull of an arm risky, we also do not start off knowing what the reward rates are for any of the arms. We have to figure this out experimentally by actually pulling on the unknown arms.\n",
    "\n",
    "Any algorithm that offers you a proposed solution to the MBP must give you a rule for selecting arms in some sequence. And this rule has to balance out your competing desires to 1) Learn about new arms and 2) earn as much reward as possible by pulling on arms your already know are good choices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf245e7",
   "metadata": {},
   "source": [
    "#### Epsilon-Greedy Algorithm :-\n",
    "- With probability 1 - epsilon, the epsilon-Greedy algorithm exploits the best known option\n",
    "- With probability epsilon /2, the epsilon-Greedy algorithm explores the best known option\n",
    "- With probability epsilon /2, the epsilon-Greedys algorithm explores the wrost known option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faf739ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a9614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy():    \n",
    "    '''\n",
    "        This class implements epsilon-Greeey Multiarm Bandit algorithm\n",
    "    '''\n",
    "    def __init__( self, epsilon, counts=[], values=[] ):\n",
    "        '''\n",
    "            epsilon : This will be a floating pointt number that tell us the frequency with which \n",
    "            we should explore one of the available arms.\n",
    "           \n",
    "            counts  : A vector of integers of length N that tells us how many time we have played \n",
    "            each of the N arms available to us in the current bandit problem.\n",
    "            \n",
    "            values  : A vector of floating point numbers that defines the average amount of reward we have gotten \n",
    "            when playing each of the N arms available to us.\n",
    "        '''\n",
    "        self.epsilon = epsilon\n",
    "        self.counts  = []\n",
    "        self.values  = []\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'EpsilonGreedy({:.2f},{!r}, {!r})'.format(self.epsilon, self.counts, self.values)\n",
    "    \n",
    "    def initialize(self, n_arms):\n",
    "        # Intializing / reset rewards & counts to zeros for each arm(or option)\n",
    "        self.counts = [ 0 for col in range(n_arms) ]\n",
    "        self.values = [ 0.0 for col in range(n_arms) ]\n",
    "        \n",
    "    def select_arm(self):\n",
    "        ''' Returns the index of the next arm to pull '''\n",
    "        \n",
    "        if random.random() > self.epsilon:\n",
    "            m = max( self.values  )            \n",
    "            if ( m > 0.0 ) :\n",
    "                return self.values.index(m)\n",
    "            else:\n",
    "                return random.randrange( len( self.values) )\n",
    "            \n",
    "        else:\n",
    "            return random.randrange( len( self.values) )\n",
    "        \n",
    "    \n",
    "    def update(self, chosen_arm, reward):\n",
    "        '''        \n",
    "        After we pull an arm, we get a reward signal back from our system. This function update our algorithm's beliefs\n",
    "        about the quality of the arm we just chose by providing this reward information.\n",
    "        \n",
    "        chosen_arm : The numeric index of the most recently chosen arm\n",
    "        reward     : The reward received from chossing that arm\n",
    "        '''\n",
    "        self.counts[ chosen_arm ] += 1\n",
    "        n = self.counts[ chosen_arm ]\n",
    "        value = self.values[chosen_arm]\n",
    "        new_value = ( ( n-1 ) * value + reward ) / float(n) \n",
    "        self.values[chosen_arm] = new_value\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cc2bdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AnnealingEpsilonGreedy():\n",
    "    \n",
    "    '''\n",
    "        This class implements epsilon-Greeey Multiarm Bandit algorithm\n",
    "    '''\n",
    "    def __init__( self, counts=[], values=[] ):\n",
    "        '''\n",
    "            epsilon : This will be a floating pointt number that tell us the frequency with which \n",
    "            we should explore one of the available arms.\n",
    "           \n",
    "            counts  : A vector of integers of length N that tells us how many time we have played \n",
    "            each of the N arms available to us in the current bandit problem.\n",
    "            \n",
    "            values  : A vector of floating point numbers that defines the average amount of reward we have gotten \n",
    "            when playing each of the N arms available to us.\n",
    "        '''\n",
    "#         self.epsilon = epsilon\n",
    "        self.counts = counts\n",
    "        self. values = values\n",
    "    def __repr__(self):\n",
    "        return 'AnnealingEpsilonGreedy({!r}, {!r})'.format( self.counts, self.values)\n",
    "\n",
    "    def updateEpsilon():\n",
    "        '''\n",
    "            Updates epsilon after each trail\n",
    "        '''\n",
    "        self.epsilon  -= (   numpy.power(self.epsilon, 4) )\n",
    "    \n",
    "    def initialize(self, n_arms):\n",
    "        # Intializing / reset rewards & counts to zeros for each arm(or option)\n",
    "        self.counts = [ 0 for col in range(n_arms) ]\n",
    "        self.values = [ 0.0 for col in range(n_arms) ]\n",
    "    \n",
    "    def select_arm(self):\n",
    "        ''' Returns the index of the next arm to pull '''\n",
    "        t = sum(self.counts) + 1\n",
    "        epsilon = 1 / math.log(t + 0.0000001 ) #\n",
    "        \n",
    "        if random.random() > epsilon:\n",
    "            m = max( self.values  )            \n",
    "            if ( m > 0.0 ) :\n",
    "                return self.values.index(m)\n",
    "            else:\n",
    "                return random.randrange( len( self.values) )\n",
    "            \n",
    "        else:\n",
    "            return random.randrange( len( self.values) )\n",
    "        \n",
    "    \n",
    "    def update(self, chosen_arm, reward):\n",
    "        '''        \n",
    "        After we pull an arm, we get a reward signal back from our system. This function update our algorithm's beliefs\n",
    "        about the quality of the arm we just chose by providing this reward information.\n",
    "        \n",
    "        chosen_arm : The numeric index of the most recently chosen arm\n",
    "        reward     : The reward received from chossing that arm\n",
    "        '''\n",
    "        self.counts[ chosen_arm ] += 1\n",
    "        n = self.counts[ chosen_arm ]\n",
    "        value = self.values[chosen_arm]\n",
    "        new_value = ( ( n-1 ) * value + reward ) / float(n) \n",
    "        self.values[chosen_arm] = new_value\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a939930f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c2d7cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilonGreedyPlots( successProbabilities, noOfTrails):\n",
    "    armsRewardForGivenTrial = lambda x : numpy.random.choice( (\"s\", \"f\"), p=[ x, 1-x])\n",
    "    totalRewardForEachEpsilon = []\n",
    "    for eEpsilon in numpy.arange(0.1, 1.1, 0.1):\n",
    "        p = EpsilonGreedy( eEpsilon, )\n",
    "        p.initialize(len(successProbabilities))\n",
    "        for ee in range(noOfTrails):\n",
    "            chosen_arm = p.select_arm() \n",
    "            p.update ( chosen_arm , 1 if armsRewardForGivenTrial(successProbabilities[chosen_arm]) == 's' else 0 )\n",
    "\n",
    "        totalReward = sum( [ i*j for i,j in zip( p.counts, p.values)] )\n",
    "\n",
    "        totalRewardForEachEpsilon.append( (p.epsilon, totalReward) )\n",
    "    return pandas.DataFrame( totalRewardForEachEpsilon, columns=[\"epsilon\", \"total_reward\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b73232",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1 )\n",
    "plt.rcParams[\"figure.figsize\"] = (30,30)\n",
    "\n",
    "noOfTrails = 500000\n",
    "\n",
    "successProbabilities = [ 0.01, 0.01 ] # Options success rates in Hindsight\n",
    "rewardsDF = epsilonGreedyPlots( successProbabilities, noOfTrails )\n",
    "plt.subplot(321)\n",
    "plt.plot(rewardsDF['epsilon'], rewardsDF['total_reward'], 'ro' )\n",
    "plt.title( f\"Figure 1: EpsilonGreedy with {successProbabilities} true reward rates\", fontsize=24)\n",
    "plt.xlabel(\"epsilon\", fontsize=24)\n",
    "plt.ylabel(\"total reward\", fontsize=24)\n",
    "plt.xticks(fontsize= 20)\n",
    "plt.yticks(fontsize= 20)\n",
    "\n",
    "#plt.show() # Depending on whether you use IPython or interactive mode, etc.\n",
    "\n",
    "successProbabilities = [ 0.01, 0.011 ] # Options success rates in Hindsight\n",
    "rewardsDF = epsilonGreedyPlots( successProbabilities, noOfTrails )\n",
    "plt.subplot(322)\n",
    "plt.plot(rewardsDF['epsilon'], rewardsDF['total_reward'], 'ro' )\n",
    "plt.title( f\"Figure 2: EpsilonGreedy with {successProbabilities} true reward rates\", fontsize=24)\n",
    "plt.xlabel(\"epsilon\", fontsize=24)\n",
    "plt.ylabel(\"total reward\", fontsize=24)\n",
    "plt.xticks(fontsize= 20)\n",
    "plt.yticks(fontsize= 20)\n",
    "\n",
    "successProbabilities = [ 0.01, 0.012 ] # Options success rates in Hindsight\n",
    "rewardsDF = epsilonGreedyPlots( successProbabilities, noOfTrails)\n",
    "plt.subplot(323)\n",
    "plt.plot(rewardsDF['epsilon'], rewardsDF['total_reward'], 'ro' )\n",
    "plt.title( f\"Figure 3: EpsilonGreedy with {successProbabilities} true reward rates\", fontsize=24)\n",
    "plt.xlabel(\"epsilon\", fontsize=24)\n",
    "plt.ylabel(\"total reward\", fontsize=24)\n",
    "plt.xticks(fontsize= 20)\n",
    "plt.yticks(fontsize= 20)\n",
    "\n",
    "successProbabilities = [ 0.01, 0.013 ] # Options success rates in Hindsight\n",
    "rewardsDF = epsilonGreedyPlots( successProbabilities, noOfTrails )\n",
    "plt.subplot(324)\n",
    "plt.plot(rewardsDF['epsilon'], rewardsDF['total_reward'], 'ro' )\n",
    "plt.title( f\"Figure 4: EpsilonGreedy with {successProbabilities} true reward rates\", fontsize=24)\n",
    "plt.xlabel(\"epsilon\", fontsize=24)\n",
    "plt.ylabel(\"total reward\", fontsize=24)\n",
    "plt.xticks(fontsize= 20)\n",
    "plt.yticks(fontsize= 20)\n",
    "\n",
    "successProbabilities = [ 0.01, 0.016 ] # Options success rates in Hindsight\n",
    "rewardsDF = epsilonGreedyPlots( successProbabilities, noOfTrails )\n",
    "plt.subplot(325)\n",
    "plt.plot(rewardsDF['epsilon'], rewardsDF['total_reward'], 'ro' )\n",
    "plt.title( f\"Figure 5: EpsilonGreedy with {successProbabilities} true reward rates\", fontsize=24)\n",
    "plt.xlabel(\"epsilon\", fontsize=24)\n",
    "plt.ylabel(\"total reward\", fontsize=24)\n",
    "plt.xticks(fontsize= 20)\n",
    "plt.yticks(fontsize= 20)\n",
    "\n",
    "successProbabilities = [ 0.01, 0.02 ] # Options success rates in Hindsight\n",
    "rewardsDF = epsilonGreedyPlots( successProbabilities, noOfTrails )\n",
    "plt.subplot(326)\n",
    "plt.plot(rewardsDF['epsilon'], rewardsDF['total_reward'], 'ro' )\n",
    "plt.title( f\"Figure 6: EpsilonGreedy with {successProbabilities} true reward rates\", fontsize=24)\n",
    "plt.xlabel(\"epsilon\", fontsize=24)\n",
    "plt.ylabel(\"total reward\", fontsize=24)\n",
    "plt.xticks(fontsize= 20)\n",
    "plt.yticks(fontsize= 20)\n",
    "plt.show() # Depending on whether you use IPython or interactive mode, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b920cda3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
